{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PwjiKG7O1THG"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import Env\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from typing import Type\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class EnvGenerator:\n",
        "    def __init__(self, env_id=\"Pendulum-v1\", render_mode=None, seed=None):\n",
        "        \"\"\"Store environment parameters.\"\"\"\n",
        "        self.env_id = env_id\n",
        "        self.render_mode = render_mode\n",
        "        self.seed = seed\n",
        "\n",
        "    def __call__(self, render_mode=None):\n",
        "        \"\"\"Create and return a new environment instance when called.\"\"\"\n",
        "        if render_mode is not None:\n",
        "            env = gym.make(self.env_id, render_mode=render_mode)\n",
        "        else:\n",
        "            env = gym.make(self.env_id, render_mode=self.render_mode)\n",
        "        if self.seed is not None:\n",
        "            env.reset(seed=self.seed)\n",
        "        return env\n",
        "\n",
        "class Actor(nn.Module): # Policy network (deterministic mapping s to a)\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.obs_in = nn.Linear(3, 200) # (x,y, ang_vel)\n",
        "        self.hidden = nn.Linear(200, 200)\n",
        "        self.a_out = nn.Linear(200, 1) # 1 action (torque)\n",
        "        self.model = nn.Sequential(\n",
        "            self.obs_in,\n",
        "            nn.ReLU(),\n",
        "            self.hidden,\n",
        "            nn.ReLU(),\n",
        "            self.a_out,\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, obs):\n",
        "        return 2 * self.model(obs)\n",
        "\n",
        "class Critic(nn.Module): # Value network (Q-function approximator)\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.obs_a_in = nn.Linear(3+1, 200) # (x,y, ang_vel, torque)\n",
        "        self.hidden = nn.Linear(200, 200)\n",
        "        self.v_out = nn.Linear(200, 1) # 1 value\n",
        "        self.model = nn.Sequential(\n",
        "            self.obs_a_in,\n",
        "            nn.ReLU(),\n",
        "            self.hidden,\n",
        "            nn.ReLU(),\n",
        "            self.v_out,\n",
        "            #nn.Tanh()  # Restrict output between [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, obs, a):\n",
        "        return self.model(torch.cat([obs, a], dim=1))\n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.buffer = []\n",
        "        self.pointer = 0\n",
        "\n",
        "    def store(self, transition):\n",
        "        if len(self.buffer) < self.size:\n",
        "            self.buffer.append(transition)\n",
        "        else:\n",
        "            self.buffer[self.pointer] = transition\n",
        "        self.pointer = (self.pointer + 1) % self.size\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "class DDPG():\n",
        "    def __init__(self, env_gen_cls: Type, hyperparam):\n",
        "        self.env_gen = env_gen_cls()\n",
        "        self.env = self.env_gen()\n",
        "        self.polyak = hyperparam['polyak']\n",
        "        self.n_epoch = hyperparam['n_epoch']\n",
        "        self.steps_per_epoch = hyperparam['steps_per_epoch']\n",
        "        self.replay_buffer_size = hyperparam['replay_buffer_size']\n",
        "        self.batch_size = hyperparam['batch_size']\n",
        "        self.gamma = hyperparam['gamma'] # reward discount factor\n",
        "        self.actor_lr = hyperparam['actor_lr']   # policy network\n",
        "        self.critic_lr = hyperparam['critic_lr'] # value network\n",
        "        self.update_after = hyperparam['update_after']\n",
        "        self.update_every = hyperparam['update_every']\n",
        "        # Uncorrelated mean zero Gaussian for exploration\n",
        "        self.add_noise = hyperparam['add_noise']\n",
        "        self.noise_decay = hyperparam['noise_decay'] # 1.0 for no noise decay, i.e. constant noise scale\n",
        "        # start steps for exploration in the beginning of training\n",
        "        self.start_steps = hyperparam['start_steps']\n",
        "\n",
        "        # Actor and critic networks\n",
        "        self.actor = Actor()\n",
        "        self.target_actor = Actor()\n",
        "        self.critic = Critic()\n",
        "        self.target_critic = Critic()\n",
        "        # Copy parameters in case parameters are not initialized the same\n",
        "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
        "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
        "        # Freeze target networks with respect to optimizers\n",
        "        for p in self.target_actor.parameters():\n",
        "            p.requires_grad = False\n",
        "        for p in self.target_critic.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=self.actor_lr)\n",
        "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=self.critic_lr)\n",
        "\n",
        "        # Replay buffer\n",
        "        self.replay_buffer = ReplayBuffer(self.replay_buffer_size)\n",
        "\n",
        "    def train(self, save_model=False):\n",
        "        # Get range of action space\n",
        "        noise_std_dev = 0.1 # (self.env.action_space.high - self.env.action_space.low) / 6\n",
        "        obs_, _ = self.env.reset()\n",
        "        max_step_count = self.n_epoch * self.steps_per_epoch\n",
        "        for step_count in range(max_step_count): # TBD: epoch?\n",
        "            # 1. Get action\n",
        "            if step_count < self.start_steps: # Encourage exploration in the beginning\n",
        "                act = self.env.action_space.sample()\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    if self.add_noise:\n",
        "                        act = self.actor(torch.as_tensor(obs_, dtype=torch.float32)) + torch.normal(0, noise_std_dev, size=(1,))\n",
        "                        act = act.cpu().detach().numpy()\n",
        "                        act = np.clip(act, self.env.action_space.low, self.env.action_space.high)\n",
        "                        if step_count % 5000 == 0:  # Decay every 5000 steps\n",
        "                            noise_std_dev *= self.noise_decay\n",
        "                    else:\n",
        "                        act = self.actor(torch.as_tensor(obs_, dtype=torch.float32))\n",
        "                        act = act.cpu().detach().numpy()\n",
        "                        act = np.clip(act, self.env.action_space.low, self.env.action_space.high)\n",
        "\n",
        "            # 2. Execute action\n",
        "            next_obs_, rew, terminated, truncated, _ = self.env.step(act)\n",
        "            done = terminated or truncated\n",
        "            # 3. Store (s, a, r, s', done) in replay buffer\n",
        "            self.replay_buffer.store((obs_, act, rew, next_obs_, 1 if done else 0))\n",
        "\n",
        "            if done:\n",
        "                obs_, _ = self.env.reset()\n",
        "            else:\n",
        "                obs_ = next_obs_\n",
        "\n",
        "            # 4. Determine if it's time to update\n",
        "            #if len(self.replay_buffer.buffer) > self.batch_size:\n",
        "            if step_count >= self.update_after and step_count % self.update_every == 0:\n",
        "                # Perform n updates\n",
        "                for _ in range(self.update_every): # TBD: n updates\n",
        "                    # 1. Sample a batch of data from replay buffer\n",
        "                    batch = self.replay_buffer.sample(self.batch_size)\n",
        "                    c_loss, a_loss = self.update(batch)\n",
        "\n",
        "            if (step_count+1) % self.steps_per_epoch == 0:\n",
        "                echo = (step_count+1) // self.steps_per_epoch\n",
        "                # TODO: Test for 5 times without render and collect the average reward\n",
        "                test_avg_rew = self.test()\n",
        "                print(f\"====== Epoch {echo}/{self.n_epoch}, avg reward = {test_avg_rew} ======\")\n",
        "\n",
        "        # Save model\n",
        "        print(\"Training done.\")\n",
        "        if save_model:\n",
        "            torch.save(self.actor.state_dict(), 'param/ddpg_actor_params2.pkl')\n",
        "            torch.save(self.critic.state_dict(), 'param/ddpg_critic_params2.pkl')\n",
        "\n",
        "    def update(self, batch_data):\n",
        "        s = torch.tensor(np.array([t[0] for t in batch_data]), dtype=torch.float).to(device)\n",
        "        a = torch.tensor(np.array([t[1] for t in batch_data]), dtype=torch.float).view(-1, 1).to(device)\n",
        "        r = torch.tensor(np.array([t[2] for t in batch_data]), dtype=torch.float).view(-1, 1).to(device)\n",
        "        s_prime = torch.tensor(np.array([t[3] for t in batch_data]), dtype=torch.float).to(device)\n",
        "        done = torch.tensor(np.array([t[4] for t in batch_data]), dtype=torch.float).view(-1, 1).to(device)\n",
        "\n",
        "        # 2. Compute Q-value from target critic network with action from target policy network\n",
        "        self.optimizer_critic.zero_grad()\n",
        "        q_eval = self.critic(s, a)\n",
        "        with torch.no_grad(): # Not updating target networks with gradients\n",
        "            q_target = r + self.gamma * (1-done) * self.target_critic(s_prime, self.target_actor(s_prime))\n",
        "\n",
        "        # 3. Update critic\n",
        "        msbe_loss = nn.MSELoss()\n",
        "        c_loss = msbe_loss(q_eval, q_target)\n",
        "        c_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1.0)\n",
        "        self.optimizer_critic.step()\n",
        "\n",
        "        # Freeze critic network for policy update\n",
        "        for p in self.critic.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # 8. Update actor (gradient ascent)\n",
        "        self.optimizer_actor.zero_grad()\n",
        "        a_loss = -self.critic(s, self.actor(s)).mean()\n",
        "        a_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)\n",
        "        self.optimizer_actor.step()\n",
        "\n",
        "        # Unfreeze critic network\n",
        "        for p in self.critic.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "        # 9. Update target networks with polyak averaging\n",
        "        with torch.no_grad():\n",
        "            for target_param, eval_param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
        "                target_param.data.copy_(self.polyak * target_param.data + (1 - self.polyak) * eval_param.data)\n",
        "            for target_param, eval_param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
        "                target_param.data.copy_(self.polyak * target_param.data + (1 - self.polyak) * eval_param.data)\n",
        "\n",
        "        # Print loss\n",
        "        #print(f\"critic loss = {c_loss.item()}, actor loss = {a_loss.item()}\")\n",
        "        return c_loss.item(), a_loss.item()\n",
        "\n",
        "    def load_model(self, actor_path, critic_path):\n",
        "        self.actor.load_state_dict(torch.load(actor_path))\n",
        "        self.critic.load_state_dict(torch.load(critic_path))\n",
        "\n",
        "    def test(self, verbose=False, render=False):\n",
        "        test_count = 5\n",
        "        total_reward = 0\n",
        "        test_env = self.env_gen(render_mode='human' if render else None)\n",
        "        for i in range(test_count):\n",
        "            obs_, _ = test_env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                with torch.no_grad():\n",
        "                    act = self.actor(torch.as_tensor(obs_, dtype=torch.float32))\n",
        "                    act = act.cpu().detach().numpy()\n",
        "                    act = np.clip(act, test_env.action_space.low, test_env.action_space.high)\n",
        "                obs_, rew, terminated, truncated, _ = test_env.step(act)\n",
        "                done = terminated or truncated\n",
        "                total_reward += rew\n",
        "                if render:\n",
        "                    test_env.render()\n",
        "                if verbose:\n",
        "                    print(f\"obs = {obs_}, act = {act}, rew = {rew}\")\n",
        "                    time.sleep(0.01)\n",
        "\n",
        "        # Average reward\n",
        "        print(f\"Average reward = {total_reward / test_count}\")\n",
        "        return total_reward / test_count\n",
        "\n",
        "    def print_model(self):\n",
        "        print(self.actor)\n",
        "        print(self.critic)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Initialize the DDPG agent with hyperparameters"
      ],
      "metadata": {
        "id": "w-A8AjWL11PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DDPG(EnvGenerator, hyperparam={\n",
        "    'n_epoch': 40,\n",
        "    'steps_per_epoch': 2000,\n",
        "    'update_after': 1000,\n",
        "    'update_every': 50,\n",
        "    'polyak': 0.995,\n",
        "    'replay_buffer_size': 10000,\n",
        "    'batch_size': 32,\n",
        "    'gamma': 0.99,\n",
        "    'actor_lr': 0.0001,\n",
        "    'critic_lr': 0.0001,\n",
        "    'add_noise': True,\n",
        "    'noise_decay': 1.0,\n",
        "    'start_steps': 1000\n",
        "})\n"
      ],
      "metadata": {
        "id": "rCv1VlX81gUV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.print_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7OebuP42EC-",
        "outputId": "0cad860b-d432-45bd-b3b0-680455c45434"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actor(\n",
            "  (obs_in): Linear(in_features=3, out_features=200, bias=True)\n",
            "  (hidden): Linear(in_features=200, out_features=200, bias=True)\n",
            "  (a_out): Linear(in_features=200, out_features=1, bias=True)\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=3, out_features=200, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=200, out_features=1, bias=True)\n",
            "    (5): Tanh()\n",
            "  )\n",
            ")\n",
            "Critic(\n",
            "  (obs_a_in): Linear(in_features=4, out_features=200, bias=True)\n",
            "  (hidden): Linear(in_features=200, out_features=200, bias=True)\n",
            "  (v_out): Linear(in_features=200, out_features=1, bias=True)\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=4, out_features=200, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=200, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Train the agent"
      ],
      "metadata": {
        "id": "b5LoHHza1w6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent.train(save_model=True)\n"
      ],
      "metadata": {
        "id": "zi8QmuT-1juH",
        "outputId": "a099bc09-f3ed-4650-a1a1-91099bf2e363",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward = -1715.262835419855\n",
            "====== Epoch 1/40, avg reward = -1715.262835419855 ======\n",
            "Average reward = -1339.0242019527045\n",
            "====== Epoch 2/40, avg reward = -1339.0242019527045 ======\n",
            "Average reward = -1097.1027358835865\n",
            "====== Epoch 3/40, avg reward = -1097.1027358835865 ======\n",
            "Average reward = -1339.9866298621116\n",
            "====== Epoch 4/40, avg reward = -1339.9866298621116 ======\n",
            "Average reward = -940.3063423499686\n",
            "====== Epoch 5/40, avg reward = -940.3063423499686 ======\n",
            "Average reward = -881.9232048093678\n",
            "====== Epoch 6/40, avg reward = -881.9232048093678 ======\n",
            "Average reward = -881.3976025864395\n",
            "====== Epoch 7/40, avg reward = -881.3976025864395 ======\n",
            "Average reward = -827.7425711621738\n",
            "====== Epoch 8/40, avg reward = -827.7425711621738 ======\n",
            "Average reward = -661.99624776927\n",
            "====== Epoch 9/40, avg reward = -661.99624776927 ======\n",
            "Average reward = -722.4577084782069\n",
            "====== Epoch 10/40, avg reward = -722.4577084782069 ======\n",
            "Average reward = -739.292429277981\n",
            "====== Epoch 11/40, avg reward = -739.292429277981 ======\n",
            "Average reward = -679.3965175999972\n",
            "====== Epoch 12/40, avg reward = -679.3965175999972 ======\n",
            "Average reward = -578.6282260162635\n",
            "====== Epoch 13/40, avg reward = -578.6282260162635 ======\n",
            "Average reward = -793.8367898825479\n",
            "====== Epoch 14/40, avg reward = -793.8367898825479 ======\n",
            "Average reward = -599.1321668273483\n",
            "====== Epoch 15/40, avg reward = -599.1321668273483 ======\n",
            "Average reward = -674.0927010437125\n",
            "====== Epoch 16/40, avg reward = -674.0927010437125 ======\n",
            "Average reward = -528.7097082195273\n",
            "====== Epoch 17/40, avg reward = -528.7097082195273 ======\n",
            "Average reward = -496.5788205396585\n",
            "====== Epoch 18/40, avg reward = -496.5788205396585 ======\n",
            "Average reward = -293.5528227191651\n",
            "====== Epoch 19/40, avg reward = -293.5528227191651 ======\n",
            "Average reward = -406.84797318537153\n",
            "====== Epoch 20/40, avg reward = -406.84797318537153 ======\n",
            "Average reward = -272.10975735996897\n",
            "====== Epoch 21/40, avg reward = -272.10975735996897 ======\n",
            "Average reward = -661.6717599141891\n",
            "====== Epoch 22/40, avg reward = -661.6717599141891 ======\n",
            "Average reward = -671.2611157743241\n",
            "====== Epoch 23/40, avg reward = -671.2611157743241 ======\n",
            "Average reward = -406.297887857702\n",
            "====== Epoch 24/40, avg reward = -406.297887857702 ======\n",
            "Average reward = -225.553384643437\n",
            "====== Epoch 25/40, avg reward = -225.553384643437 ======\n",
            "Average reward = -337.99457235010084\n",
            "====== Epoch 26/40, avg reward = -337.99457235010084 ======\n",
            "Average reward = -916.2985503908164\n",
            "====== Epoch 27/40, avg reward = -916.2985503908164 ======\n",
            "Average reward = -299.35886530265117\n",
            "====== Epoch 28/40, avg reward = -299.35886530265117 ======\n",
            "Average reward = -206.707496748086\n",
            "====== Epoch 29/40, avg reward = -206.707496748086 ======\n",
            "Average reward = -100.40167668388946\n",
            "====== Epoch 30/40, avg reward = -100.40167668388946 ======\n",
            "Average reward = -167.56694049987138\n",
            "====== Epoch 31/40, avg reward = -167.56694049987138 ======\n",
            "Average reward = -172.34006101311755\n",
            "====== Epoch 32/40, avg reward = -172.34006101311755 ======\n",
            "Average reward = -125.31419597148995\n",
            "====== Epoch 33/40, avg reward = -125.31419597148995 ======\n",
            "Average reward = -183.11738571437502\n",
            "====== Epoch 34/40, avg reward = -183.11738571437502 ======\n",
            "Average reward = -1420.8640126877433\n",
            "====== Epoch 35/40, avg reward = -1420.8640126877433 ======\n",
            "Average reward = -189.29526199555963\n",
            "====== Epoch 36/40, avg reward = -189.29526199555963 ======\n",
            "Average reward = -119.15761392531871\n",
            "====== Epoch 37/40, avg reward = -119.15761392531871 ======\n",
            "Average reward = -161.83798794871424\n",
            "====== Epoch 38/40, avg reward = -161.83798794871424 ======\n",
            "Average reward = -187.1032591289515\n",
            "====== Epoch 39/40, avg reward = -187.1032591289515 ======\n",
            "Average reward = -145.25816074001517\n",
            "====== Epoch 40/40, avg reward = -145.25816074001517 ======\n",
            "Training done.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Parent directory param does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-96ccd8b0f96a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-cd94924544e9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, save_model)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training done.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'param/ddpg_actor_params2.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'param/ddpg_critic_params2.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m             _save(\n\u001b[1;32m    851\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Parent directory param does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Test the agent"
      ],
      "metadata": {
        "id": "R7Nkejjv1mMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent.test(verbose=True, render=True)\n"
      ],
      "metadata": {
        "id": "oxSFgci61kk0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}