{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PwjiKG7O1THG"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import Env\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from typing import Type\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class EnvGenerator:\n",
        "    def __init__(self, env_id=\"Pendulum-v1\", render_mode=None, seed=None):\n",
        "        \"\"\"Store environment parameters.\"\"\"\n",
        "        self.env_id = env_id\n",
        "        self.render_mode = render_mode\n",
        "        self.seed = seed\n",
        "\n",
        "    def __call__(self, render_mode=None):\n",
        "        \"\"\"Create and return a new environment instance when called.\"\"\"\n",
        "        if render_mode is not None:\n",
        "            env = gym.make(self.env_id, render_mode=render_mode)\n",
        "        else:\n",
        "            env = gym.make(self.env_id, render_mode=self.render_mode)\n",
        "        if self.seed is not None:\n",
        "            env.reset(seed=self.seed)\n",
        "        return env\n",
        "\n",
        "class Actor(nn.Module): # Policy network (deterministic mapping s to a)\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.obs_in = nn.Linear(3, 200) # (x,y, ang_vel)\n",
        "        self.hidden = nn.Linear(200, 200)\n",
        "        self.a_out = nn.Linear(200, 1) # 1 action (torque)\n",
        "        self.model = nn.Sequential(\n",
        "            self.obs_in,\n",
        "            nn.ReLU(),\n",
        "            self.hidden,\n",
        "            nn.ReLU(),\n",
        "            self.a_out,\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, obs):\n",
        "        return 2 * self.model(obs)\n",
        "\n",
        "class Critic(nn.Module): # Value network (Q-function approximator)\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.obs_a_in = nn.Linear(3+1, 200) # (x,y, ang_vel, torque)\n",
        "        self.hidden = nn.Linear(200, 200)\n",
        "        self.v_out = nn.Linear(200, 1) # 1 value\n",
        "        self.model = nn.Sequential(\n",
        "            self.obs_a_in,\n",
        "            nn.ReLU(),\n",
        "            self.hidden,\n",
        "            nn.ReLU(),\n",
        "            self.v_out,\n",
        "            #nn.Tanh()  # Restrict output between [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, obs, a):\n",
        "        return self.model(torch.cat([obs, a], dim=1))\n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.buffer = []\n",
        "        self.pointer = 0\n",
        "\n",
        "    def store(self, transition):\n",
        "        if len(self.buffer) < self.size:\n",
        "            self.buffer.append(transition)\n",
        "        else:\n",
        "            self.buffer[self.pointer] = transition\n",
        "        self.pointer = (self.pointer + 1) % self.size\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "class DDPG():\n",
        "    def __init__(self, env_gen_cls: Type, hyperparam):\n",
        "        self.env_gen = env_gen_cls()\n",
        "        self.env = self.env_gen()\n",
        "        self.polyak = hyperparam['polyak']\n",
        "        self.n_epoch = hyperparam['n_epoch']\n",
        "        self.steps_per_epoch = hyperparam['steps_per_epoch']\n",
        "        self.replay_buffer_size = hyperparam['replay_buffer_size']\n",
        "        self.batch_size = hyperparam['batch_size']\n",
        "        self.gamma = hyperparam['gamma'] # reward discount factor\n",
        "        self.actor_lr = hyperparam['actor_lr']   # policy network\n",
        "        self.critic_lr = hyperparam['critic_lr'] # value network\n",
        "        self.update_after = hyperparam['update_after']\n",
        "        self.update_every = hyperparam['update_every']\n",
        "        # Uncorrelated mean zero Gaussian for exploration\n",
        "        self.add_noise = hyperparam['add_noise']\n",
        "        self.noise_decay = hyperparam['noise_decay'] # 1.0 for no noise decay, i.e. constant noise scale\n",
        "        # start steps for exploration in the beginning of training\n",
        "        self.start_steps = hyperparam['start_steps']\n",
        "\n",
        "        # Actor and critic networks\n",
        "        self.actor = Actor()\n",
        "        self.target_actor = Actor()\n",
        "        self.critic = Critic()\n",
        "        self.target_critic = Critic()\n",
        "        # Copy parameters in case parameters are not initialized the same\n",
        "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
        "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
        "        # Freeze target networks with respect to optimizers\n",
        "        for p in self.target_actor.parameters():\n",
        "            p.requires_grad = False\n",
        "        for p in self.target_critic.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=self.actor_lr)\n",
        "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=self.critic_lr)\n",
        "\n",
        "        # Replay buffer\n",
        "        self.replay_buffer = ReplayBuffer(self.replay_buffer_size)\n",
        "\n",
        "    def train(self, save_model=False):\n",
        "        # Get range of action space\n",
        "        noise_std_dev = 0.1 # (self.env.action_space.high - self.env.action_space.low) / 6\n",
        "        obs_, _ = self.env.reset()\n",
        "        max_step_count = self.n_epoch * self.steps_per_epoch\n",
        "        for step_count in range(max_step_count): # TBD: epoch?\n",
        "            # 1. Get action\n",
        "            if step_count < self.start_steps: # Encourage exploration in the beginning\n",
        "                act = self.env.action_space.sample()\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    if self.add_noise:\n",
        "                        act = self.actor(torch.as_tensor(obs_, dtype=torch.float32)) + torch.normal(0, noise_std_dev, size=(1,))\n",
        "                        act = act.cpu().detach().numpy()\n",
        "                        act = np.clip(act, self.env.action_space.low, self.env.action_space.high)\n",
        "                        if step_count % 5000 == 0:  # Decay every 5000 steps\n",
        "                            noise_std_dev *= self.noise_decay\n",
        "                    else:\n",
        "                        act = self.actor(torch.as_tensor(obs_, dtype=torch.float32))\n",
        "                        act = act.cpu().detach().numpy()\n",
        "                        act = np.clip(act, self.env.action_space.low, self.env.action_space.high)\n",
        "\n",
        "            # 2. Execute action\n",
        "            next_obs_, rew, terminated, truncated, _ = self.env.step(act)\n",
        "            done = terminated or truncated\n",
        "            # 3. Store (s, a, r, s', done) in replay buffer\n",
        "            self.replay_buffer.store((obs_, act, rew, next_obs_, 1 if done else 0))\n",
        "\n",
        "            if done:\n",
        "                obs_, _ = self.env.reset()\n",
        "            else:\n",
        "                obs_ = next_obs_\n",
        "\n",
        "            # 4. Determine if it's time to update\n",
        "            #if len(self.replay_buffer.buffer) > self.batch_size:\n",
        "            if step_count >= self.update_after and step_count % self.update_every == 0:\n",
        "                # Perform n updates\n",
        "                for _ in range(self.update_every): # TBD: n updates\n",
        "                    # 1. Sample a batch of data from replay buffer\n",
        "                    batch = self.replay_buffer.sample(self.batch_size)\n",
        "                    c_loss, a_loss = self.update(batch)\n",
        "\n",
        "            if (step_count+1) % self.steps_per_epoch == 0:\n",
        "                echo = (step_count+1) // self.steps_per_epoch\n",
        "                # TODO: Test for 5 times without render and collect the average reward\n",
        "                test_avg_rew = self.test()\n",
        "                print(f\"====== Epoch {echo}/{self.n_epoch}, avg reward = {test_avg_rew} ======\")\n",
        "\n",
        "        # Save model\n",
        "        print(\"Training done.\")\n",
        "        if save_model:\n",
        "            torch.save(self.actor.state_dict(), 'param/ddpg_actor_params2.pkl')\n",
        "            torch.save(self.critic.state_dict(), 'param/ddpg_critic_params2.pkl')\n",
        "\n",
        "    def update(self, batch_data):\n",
        "        s = torch.tensor(np.array([t[0] for t in batch_data]), dtype=torch.float).to(device)\n",
        "        a = torch.tensor(np.array([t[1] for t in batch_data]), dtype=torch.float).view(-1, 1).to(device)\n",
        "        r = torch.tensor(np.array([t[2] for t in batch_data]), dtype=torch.float).view(-1, 1).to(device)\n",
        "        s_prime = torch.tensor(np.array([t[3] for t in batch_data]), dtype=torch.float).to(device)\n",
        "        done = torch.tensor(np.array([t[4] for t in batch_data]), dtype=torch.float).view(-1, 1).to(device)\n",
        "\n",
        "        # 2. Compute Q-value from target critic network with action from target policy network\n",
        "        self.optimizer_critic.zero_grad()\n",
        "        q_eval = self.critic(s, a)\n",
        "        with torch.no_grad(): # Not updating target networks with gradients\n",
        "            q_target = r + self.gamma * (1-done) * self.target_critic(s_prime, self.target_actor(s_prime))\n",
        "\n",
        "        # 3. Update critic\n",
        "        msbe_loss = nn.MSELoss()\n",
        "        c_loss = msbe_loss(q_eval, q_target)\n",
        "        c_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1.0)\n",
        "        self.optimizer_critic.step()\n",
        "\n",
        "        # Freeze critic network for policy update\n",
        "        for p in self.critic.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # 8. Update actor (gradient ascent)\n",
        "        self.optimizer_actor.zero_grad()\n",
        "        a_loss = -self.critic(s, self.actor(s)).mean()\n",
        "        a_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)\n",
        "        self.optimizer_actor.step()\n",
        "\n",
        "        # Unfreeze critic network\n",
        "        for p in self.critic.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "        # 9. Update target networks with polyak averaging\n",
        "        with torch.no_grad():\n",
        "            for target_param, eval_param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
        "                target_param.data.copy_(self.polyak * target_param.data + (1 - self.polyak) * eval_param.data)\n",
        "            for target_param, eval_param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
        "                target_param.data.copy_(self.polyak * target_param.data + (1 - self.polyak) * eval_param.data)\n",
        "\n",
        "        # Print loss\n",
        "        #print(f\"critic loss = {c_loss.item()}, actor loss = {a_loss.item()}\")\n",
        "        return c_loss.item(), a_loss.item()\n",
        "\n",
        "    def load_model(self, actor_path, critic_path):\n",
        "        self.actor.load_state_dict(torch.load(actor_path))\n",
        "        self.critic.load_state_dict(torch.load(critic_path))\n",
        "\n",
        "    def test(self, verbose=False, render=False):\n",
        "        test_count = 5\n",
        "        total_reward = 0\n",
        "        test_env = self.env_gen(render_mode='human' if render else None)\n",
        "        for i in range(test_count):\n",
        "            obs_, _ = test_env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                with torch.no_grad():\n",
        "                    act = self.actor(torch.as_tensor(obs_, dtype=torch.float32))\n",
        "                    act = act.cpu().detach().numpy()\n",
        "                    act = np.clip(act, test_env.action_space.low, test_env.action_space.high)\n",
        "                obs_, rew, terminated, truncated, _ = test_env.step(act)\n",
        "                done = terminated or truncated\n",
        "                total_reward += rew\n",
        "                if render:\n",
        "                    test_env.render()\n",
        "                if verbose:\n",
        "                    print(f\"obs = {obs_}, act = {act}, rew = {rew}\")\n",
        "                    time.sleep(0.01)\n",
        "\n",
        "        # Average reward\n",
        "        print(f\"Average reward = {total_reward / test_count}\")\n",
        "        return total_reward / test_count\n",
        "\n",
        "    def print_model(self):\n",
        "        print(self.actor)\n",
        "        print(self.critic)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Initialize the DDPG agent with hyperparameters"
      ],
      "metadata": {
        "id": "w-A8AjWL11PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DDPG(EnvGenerator, hyperparam={\n",
        "    'n_epoch': 40,\n",
        "    'steps_per_epoch': 2000,\n",
        "    'update_after': 1000,\n",
        "    'update_every': 50,\n",
        "    'polyak': 0.995,\n",
        "    'replay_buffer_size': 10000,\n",
        "    'batch_size': 32,\n",
        "    'gamma': 0.99,\n",
        "    'actor_lr': 0.0001,\n",
        "    'critic_lr': 0.0001,\n",
        "    'add_noise': True,\n",
        "    'noise_decay': 1.0,\n",
        "    'start_steps': 1000\n",
        "})\n",
        ""
      ],
      "metadata": {
        "id": "rCv1VlX81gUV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.print_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7OebuP42EC-",
        "outputId": "77836adf-a000-44bd-84bf-fadc2bbe8042"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actor(\n",
            "  (obs_in): Linear(in_features=3, out_features=200, bias=True)\n",
            "  (hidden): Linear(in_features=200, out_features=200, bias=True)\n",
            "  (a_out): Linear(in_features=200, out_features=1, bias=True)\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=3, out_features=200, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=200, out_features=1, bias=True)\n",
            "    (5): Tanh()\n",
            "  )\n",
            ")\n",
            "Critic(\n",
            "  (obs_a_in): Linear(in_features=4, out_features=200, bias=True)\n",
            "  (hidden): Linear(in_features=200, out_features=200, bias=True)\n",
            "  (v_out): Linear(in_features=200, out_features=1, bias=True)\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=4, out_features=200, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=200, out_features=200, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=200, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Train the agent"
      ],
      "metadata": {
        "id": "b5LoHHza1w6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent.train(save_model=True)\n",
        ""
      ],
      "metadata": {
        "id": "zi8QmuT-1juH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Test the agent"
      ],
      "metadata": {
        "id": "R7Nkejjv1mMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    agent.test(verbose=True, render=True)\n"
      ],
      "metadata": {
        "id": "oxSFgci61kk0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}